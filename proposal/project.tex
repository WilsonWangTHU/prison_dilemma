\documentclass{article}
\usepackage{nips_2017}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}      % microtypography

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{subcaption}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{Computational Behaviors of Learning Agents in Social Dilemmas}

\author{
  Tingwu Wang\\
  Department of Computer Science\\
  University of Toronto\\
  \texttt{tingwuwang@cs.toronto.edu} \\
}

\begin{document}
\maketitle
\begin{abstract}
Most Reinforcement Learning (RL) systems have a single-agent's view,
where the agent tries to optimize its' expected discounted total reward,
    and the other agents in the environment is treated as part of the environment.
However, multi-agent assumption is crucial for building artificial intelligence,
since interactions, and more importantly,
competitions between different agents are very common in real-world applications.
Individuals have the temptation to increase payoffs, which at the same time
might endanger the total welfare. %
Recently cooperations in social dilemmas of multi-agent are receiving more attention.
Traditionally, the convergence and asymptotic behaviors of two players
in a general sum games have been intensively studied.
However, most of the studies have been focused on simple general sum matrix games,
where the dynamics of the environment is known or easy to model.
In reality, agents might take multiple time-steps before a reward signal is given
in an environment with complex dynamics,
and opponents' strategies are hard to interpret.
In this paper, we empirically study the computational behaviors of agents during learning,
and proposed Cooperative Mutual Assured Retaliation (CMAR) agents,
which are able to maintain cooperation in certain complex social dilemmas.
\end{abstract}

\section{Introduction}
Reinforcement Learning has been very successful for numerous single-agent tasks, which includes playing video games~\cite{mnih2015human},
chess~\cite{silver2016mastering,silver2017mastering},
as well as locomotion controls~\cite{gae,trpo,ppo}.
Less attention has been paid to multi-agent systems until recently.
Most multi-agent RL problems could be classified into two categories by their type of goals,
which are namely shared-goal problems and independent-goal problems.
In~\cite{tuyls2018symmetric,foerster2017learning,sukhbaatar2016learning},
researchers study the multi-agent systems where an agent could optimize the shared reward of the community by communicating with other agents in the environment.
% One of the shared-goal problems' most major concern is the efficiency of communication.
In independent-goal systems~\cite{lerer2017maintaining,leibo2017multi},
the assumption is that other agents in the environment could either be cooperative,
malicious or selfish.
The agent being trained will have to monitor the status of all other agents,
and use different policies accordingly. 
In~\cite{al2017continuous}, agents are put into a competitive environment,
and agents compete against each other to gain better utility.

Previously, people studying social dilemma have been focusing mostly on simple general sum matrix games~\cite{tullock1974social}.
Among them, repeated general-sum matrix game with social dilemma is one of the most classic models~\cite{bouzy2010multi, leibo2017multi}.
Typically, a common choice of policies for most agents is Nash equilibrium~\cite{maskin1999Nash, tullock1974social, bouzy2010multi},
in which each player has no gain by changing only their own strategy.

However, previous works have not answered the following questions:
First of all, tasks in real life are arbitrary, which means that
the dominant strategy or Nash equilibrium might not exist or hard to discover.
There is also the difficulty of understanding opponent's intrinsic intention.
In prisoner's dilemma~\cite{nowak1993strategy}, the defecting actions is self-explanatory.
However in real life, agents need an inference module to model whether the opponent is defecting.
In most literature, we assume that there is unlimited computation resource for every agent.
But computationally, to train an agent,
the limitation on both the model's capacity and computation power will inevitably affect agents' behaviors.
And since the dynamics is unknown or hard to model,
the policy has to be learned from past experiences, which involves exploration.
Opponent's policy distribution is unstatic,
which affects and is affected by agent's current policy at the same time.

The contributions of this paper is three-fold\footnote{Code is available in the project page \url{https://github.com/WilsonWangTHU/prison_dilemma}}, 
\begin{itemize}
\item We provide empirical analysis of computational behaviors of different agents.
A mixture of trainable agents and agents of pre-defined strategy is tested on general social dilemmas.
\item We provide benchmarking environments for complex social dilemmas.
Multiple steps of wide range of choices of actions are needed before the reward is received.
\item We design CMAR agents, which could maintain cooperation in complex social dilemmas.
\end{itemize}
% Currently, most settings of the literatures of RL in multi-agent system are limited in that,
% either there is only forced cooperations,
% or the other agents' behaviors could be easily identified if they are not cooperating.
% We argue that however, in many real-world applications or scenarios where a collective decision is needed for agents,
% it is impossible or too costly to tell if one of the other agents is lying or not.
% More specifically, we are interested in a strategy-proof system,
% where all the reasonable agents, whether cooperative or selfish,
% will converge to the policy where the total utility of the system is maximized as well as possible.
% Inspired by algorithmic mechanism design~\cite{alon2010strategyproof,procaccia2009approximate}, we notice that it is possible to obtain truthfulness without resorting to payments,
% which suits the settings of multi-agent RL problems.
% 
% We show (are going to show) that by combining approximate mechanism design and reward shaping~\cite{ng1999policy},
% we are able to design strategy-proof reward functions for RL agents.
\section{Preliminary}
\subsection{Reinforcement Learning}
To optimize the policy for each agent, we implicit model other opponents and the environment
and treat the problem for every agent as a separate Markov Decision Process.
Starting from the current state, the agent chooses an action based on the observation.
The environment receives the actions from all agents and a transition to the next state is made.
This process is repeated until the episode ends.
We define the MDP to be a tuple of
\((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})\),
where \(\mathcal{S}\) denotes the state space of our problem,
\(\mathcal{A}\) denotes the action space of the agents.
\(\mathcal{P}\) denotes the transition probability distribution,
which could be written as \(p(S', S, a)\).
Distribution \(p(S', S, a) \rightarrow \mathbb{R}\) is not known by the agent in advance.
Since multiple agents are in the environment, \(p(S', S, a)\) is dependent on other agents' policy.
\(\mathcal{R}\) similarly describes the reward generated by the environment
\(r(S) \rightarrow \mathbb{R}\).
More specifically, we use the notation of \(\gamma\) to describe the discount factor and
\(s^{t=0}\) as the initial state, which follows the distribution of starting states \(p_0(s^{t=0})\).
The expected total reward is the objective function for the agent to optimize, which is
\begin{equation}\label{equation:objective}
    \eta(\pi) = \mathbb{E}_{s^{t=0}}\left[\sum_{t=0}^{T=\infty}\gamma^t r(s_t)\right].
\end{equation}
In this paper,
we use stochastic policy gradient method~\cite{sutton2000}.
The stochastic policy gradient are formulated as follow:
\begin{equation}\label{spg}
\begin{aligned}
 \nabla_\theta\mathcal{J}(\pi) =
    \mathbb{E}_{s^t=0\sim p_0(s^{t=0}),\,a\sim\pi} \left[\nabla_\theta\log\pi(a^t|s^t) Q_\pi(s^t, a^t)\right],
    \end{aligned}
\end{equation}
where \(Q_\pi(s^t, a^t)\) is the state-action value.
\subsection{Social Dilemmas}
The framework of this paper is limited in social dilemmas in repeated general-sum games,
which is a generalization of repeated general-sum matrix games.

One of the most classic example is Prisoner's Dilemma.
An agent could choose to cooperate or defect.
Following the notation from~\cite{macy2002learning, leibo2017multi},
the four possible outcomes are $R$ (reward of mutual cooperation),
$P$ (punishment arising from mutual defection),
$S$ (sucker outcome obtained by the player who cooperates with a defecting partner),
and $T$ (temptation outcome achieved by defecting against a cooperator).

And social dilemma in our paper is the case when the outcomes of the game
satisfy the following \emph{social dilemma inequalities} from \cite{macy2002learning}.
$R > P$; mutual cooperation is preferred to mutual defection.
$R > S$; mutual cooperation is preferred to being exploited by a defector.
$2R > T + S$;
mutual cooperation is preferred to an equal probability of unilateral cooperation and defection.
And either $T > R$; exploiting a cooperator is preferred over mutual cooperation,
or $P > S$; mutual defection is preferred over being exploited.

\section{Problem Formulation and CMAR Agents}

% There are two modules in our model: the reward-assigner and the agents.
% The agents optimize its reward by interacting with the environment, namely the reward-assigner,
% while the reward-assigner is trying to increasing the total utility of the system by
% guiding the agents' behavior with a reward function.

\subsection{Formulation of Agents}
In this paper, we formulate several types of agent, which includes Naive agents, selfish agents and our proposed CMAR agents.
\paragraph{Naive Agents}
Naive agents always try to optimize the total reward of both agents, i.e., the objective function for naive agents is
\begin{equation}\label{equation:naive_objective}
    \eta_N(\pi) =
    \mathbb{E}_{s^{t=0}}\left[\sum_{t=0}^{T=\infty}\gamma^t \left(r_N(s_t) + r_O(s_t)\right)\right],
\end{equation}
where \(r_N(s_t),\, r_O(s_t)\) are respectively the reward of the naive agent and its opponents.
\paragraph{Selfish Agents}
Selfish agents are truly rational agents.
These agents only optimizing the total reward for itself,
and therefore they are always looking for dominant strategy.
Ideally, they might converge to Nash equilibrium if there exists one.
We denote the reward for selfish agent as \(r_S(s_t)\), and therefore
\begin{equation}\label{equation:self_objective}
    \eta_S(\pi) =
    \mathbb{E}_{s^{t=0}}\left[\sum_{t=0}^{T=\infty}\gamma^t r_S(s_t)\right].
\end{equation}
\paragraph{Adaptive Agents}
Dynamically adapting its behavior so that the total number of cooperation is maximized.
One of the classic Adaptive agent is TFT Agents~\cite{nowak1993strategy}.
An ideal adaptive agent will first cooperate,
then subsequently replicate the opponent's previous action.
\begin{algorithm}[!t]
\caption{Training of CMAR}\label{algo}
\begin{algorithmic}[1]
\State \textbf{Input:}
    \(\theta_R, \theta_C, \theta_I\) respectively the initial weight parameters for retaliation and cooperation policy.

\While{Iteration not finished}
    \State Start one episode of game

    \While{Game not finished}
        Generate actions from the distribution of two policies
        \begin{equation}
            a_C(s_t) = \pi_{\theta_C}(s_t), \, a_R(s_t) = \pi_{\theta_C}(s_t)
        \end{equation}

        \State If Opponent's cooperating according to the Inference Model \(\mbox{Inf}_{\theta_I}\), apply \(a_t = a_c(s_t)\)
        \State Else apply \(a_t = a_R(s_t)\)

    \EndWhile

    \State Update retaliation policy objective using samples from the game played
        \begin{equation}
            \begin{aligned}
                \eta_{R}S(\pi) = \mathbb{E}_{s^{t=0}}\left[\sum_{t=0}^{T=\infty}-\gamma^tr_O(s_t)\right],\,\,
                \eta_{C}S(\pi) = \mathbb{E}_{s^{t=0}}\left[\sum_{t=0}^{T=\infty}\gamma^t(r_C(s_t) + r_O(s_t))\right].
            \end{aligned}
        \end{equation}

\EndWhile\label{euclidendwhile}
\end{algorithmic}
\end{algorithm}
\subsection{CMAR Agents}
Inspired by TFT (Tit for Tat) agent, we would like an agent to start with cooperation,
but have the ability to maintain cooperation.
Also inspired by Mutual Assured Destruction~\cite{ni2014mutually},
i.e., the threat of using strong weapons against the enemy
prevents the enemy's use of those same weapons,
we notice that the threat of retaliation if the opponent is not cooperating will
essentially result in full cooperation through the game.
We build the Cooperative Mutual Assured Retaliation (CMAR) agents,
by applying the following Cooperative-Mutual-Assured-Retaliation behavior policy.
First of all, unlike naive agents,
CMAR agent maintains two target policies by off-policy learning~\cite{precup2001off}.
One CMAR agent maintains the cooperation target policy for optimizing total reward of both player,
and at the same time
maintains the retaliation policy for optimizing the negative of expected reward of opponent.
The agent start with cooperating,
and the agent runs an inference model from the distribution of behavior policy
of the opponent to classify if the opponent is cooperating.
If this policy is close to the distribution of CMAR's target cooperation policy,
CMAR will use cooperative policy.
Otherwise CMAR will use the retaliation policy.
To model the inference model, we could either use simple policy matching,
or we could use a GAN~\cite{goodfellow2014generative},
where we treat the CMAR's policy as postive and the opponents' policy as negative.

\begin{theorem}\label{t1}
    In General Cooperation Game with Interruption (GCGI)~\ref{appd1},
    given that the CMAR has perfect cooperation target policy and retaliation policy,
    the best policy for selfish opponent is cooperation.
\end{theorem}
The Proof for Theorem~\ref{t1} is listed in appendix~\ref{appt1}.
Note that we define GCGI in the appendix~\ref{appd1},
and most of games satisfy this constraints and could be regarded as GCGI,

\begin{theorem}\label{t2}
    CMAR's cooperation target policy and retaliation policy will both converge if convergence is true for the original adaptive agent.
\end{theorem}
Proof is given in appendix~\ref{appt2}.
Note that Off-policy learning converges under the given assumption~\cite{precup2001off}.
The algorithm of CMAR is shown in the~\ref{algo}
\section{Experiments}
\subsection{Environment Design}
We design two environments to test the performance.
In the Iterative Prisoner's Dilemma (IPD), multiple time-step of prisoner's dilemma is played.
In the Hunting Maze Dilemma (HMD), two hunters are put in a maze.
Similar to the prisoner's dilemma, two hunter could hunt their own assigned prey, which is essentially cooperation.
There is another additional prey (A-Prey) that has higher value.
If one of the hunter hunts the A-Prey,
it will interrupt the other agents' hunting and hurt the total utility of hunters.
If both of the hunters hunt the A-Prey,
then both of them will get zero reward because the A-Prey will flee.
\begin{figure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/sn_action.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/sn_reward.pdf}
(b)
\end{subfigure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ss_action.pdf}
(c)
\end{subfigure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ss_reward.pdf}
(d)
\end{subfigure}
\caption{The action distribution and reward of selfish agents against themselves and selfish agents against naive agents during training.}
\label{fig:selfish}
\end{figure}

\subsection{Results}
In figure~\ref{fig:selfish},
we show the results of the case where a selfish agent competes against another selfish agent
and the case where selfish agents are competing against naive agents.
Clearly from the figure, we know that when a selfish agent competes with a naive agent,
the total utility of the naive agent drops quickly,
since the naive agent will always choose to cooperate.
The selfish agents learn gradually to increase the number of defecting,
and always chooses to defect in the end.
One interesting result is from sub-figure (c) and (d),
when the game is played between two selfish agents.
Although both players are trying to optimize their own reward,
due to the fact that defecting is the dominant strategy in this prisoner's dilemma,
the total reward as well as both agents' expected reward are dropping with the training.\\

In figure~\ref{fig:cmar},
we validate that our designed CMAR agents are able to maintain the cooperation between themselves.
In sub-figure (a) and (d),
we can see that CMAR agents gradually increase their number of cooperation with the training
and converge to the optimal for both players.
From sub-figure (b) and (e), we notice that CMAR agents are able to cooperate with naive agents.
Further more, from sub-figure (c) and (f), we can see that by using the CMAR policy,
the CMAR agents are able to force selfish agent to cooperate,
and achieve better utility for both players.

In figure~\ref{fig:maze},
we study the computational behaviors under the more challenging Hunting Maze task.
From the figure,
we verify that CMAR agents are still able to maintain cooperation in complex environment.
Similar to the Iterative Prisoner's Dilemma,
the CMAR agents is able to maintain cooperation with themselves as well as selfish agents.
While at the same time, selfish agents will end up with defecting each other in the end.
\begin{figure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/aa_action.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/an_action.pdf}
(b)
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/as_action.pdf}
(c)
\end{subfigure}

\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/aa_reward.pdf}
(d)
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/an_reward.pdf}
(e)
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/as_reward.pdf}
(f)
\end{subfigure}
    \caption{The action distribution and reward of CMAR agents against themselves (sub-figure (a), (d)) and CMAR agents against naive agents (sub-figure (b), (e)) and CMAR agents against selfish agents (sub-figure (c), (f)) during training.}
\label{fig:cmar}
\end{figure}

\begin{figure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/maze_aa_reward.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/maze_ss_reward.pdf}
(b)
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/maze_as_reward.pdf}
(c)
\end{subfigure}
    \caption{
The reward of CMAR agents against themselves (sub-figure (a)),
selfish agents against themselves (sub-figure (b)),
    and selfish agents against CMAR agents during training in the maze environment.}
\label{fig:maze}
\end{figure}
\section{Conclusion}
In this paper, we empirically show the computational behaviors of agents during learning.
The proposed CMAR agent is able to maintain cooperation in complex social dilemmas.
Future works might involve more complex modeling of the inference model on the opponent,
and test the algorithm under even more complex settings.
\bibliography{project}
\bibliographystyle{plain}
\newpage
\appendix
\section{Proof of CMAR Agents' Property}
\subsection{Definition of GCGI}
\begin{definition}\label{appd1}
We name the following game \emph{General Cooperation Game with Interruption (GCGI)}.
In the GCGI game, the game output is dependent on all the steps through the game,
    and the length of the game is not known by the agents in advance.
    To defect,
    at least \(M>1\) steps of malicious behavior is needed before the game output is produced,
    and the other agent could switch to defecting policy within \(M - 1\) time-steps,
\end{definition}
Most of games satisfy the GCGI constraints,
which includes the two environment studied in this paper.

\subsection{Proof of Theorem~\ref{t1}}\label{appt1}
    In General Cooperation Game with Interruption (GCGI)~\ref{appd1},
    given that the CMAR has perfect cooperation target policy and retaliation policy,
    the best policy for selfish opponent is cooperation.\\\\
\textbf{Proof}:
Without loss of generality,
we assume that for the selfish agent
one arbitrary possible segment of path of length \(T\) in the game is denoted as \(\tau^T\).
We show by contradiction that this \(\tau^T\) is not the result of a game
where CMAR cooperates and selfish agent defects.

\begin{itemize}
\item \textbf{Case One}:
Assume that at time-step \(k<T-1\), the selfish choose to defect, then at time-step \(k + 1\),
the CMAR agent will choose to defect.
Therefore the it is not the case where CMAR cooperates and selfish agent defects.

\item \textbf{Case Two}:
Now assume that at time-step \(k=T-1\), the selfish choose to defect,
and the selfish agent has already chosen to defect in a earlier time-step \(k' < k\).
Then looking at time-step \(k'\), we know that again that the CMAR agent will choose to defect.

\item \textbf{Case Three}:
Now assume that at time-step \(k=T-1\), the selfish choose to defect,
and the selfish agent has not chosen to defect in a earlier time-step \(\forall k' < k\).
Since at least \(M>1\) steps of malicious behavior is needed,
the selfish agent is not able to accumulate enough steps to defect.
\end{itemize}
Thus we proof the Theorem~\ref{t1}
\subsection{Proof of Theorem~\ref{t2}}\label{appt2}
Since both the retaliation policy and cooperation policy could be trained separately from off-policy,
therefore CMAR's cooperation target policy and retaliation policy will both converge if convergence is true for the original adaptive agent.
\subsection{Miscellaneous Results}
\begin{figure}[!h]
    \centering
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/SP_0001reward.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/SP_0001action.pdf}
(b)
\end{subfigure}
    \caption{The reward and action distribution of CMAR agents against selfish agents with smaller learning rate (0.0001).}
\label{fig:misc-SP}
\end{figure}

\begin{figure}
    \centering
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ss_lr_00001reward.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ss_lr_00001action.pdf}
(b)
\end{subfigure}
    \caption{The reward and action distribution of selfish agents against selfish agents with smaller learning rate (0.0001). Smaller learning might result in the agents inability of mastering the dominant strategy.}
\label{fig:misc-SS}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ss_lr_001reward.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ss_lr_001action.pdf}
(b)
\end{subfigure}
    \caption{The reward and action distribution of selfish agents against selfish agents with bigger learning rate (0.01).}
\label{fig:misc-SSb}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ANlr001_action.pdf}
(a)
\end{subfigure}
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figure/ANlr001_action.pdf}
(b)
\end{subfigure}
    \caption{The reward and action distribution of CMAR agents against naive agents with smaller learning rate (0.01). The training converges very fast, but bigger learning rate might results in unstable behavior of the agents.}
\label{fig:misc-an}
\end{figure}
\end{document}
